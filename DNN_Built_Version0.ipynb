{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import xlrd\n",
    "import copy\n",
    "from time import *\n",
    "import  math\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取输入输出数据\n",
    "#读取输入数据集\n",
    "large_input_all_np=np.zeros((5373000,8))\n",
    "for i in range (1,15):\n",
    "    large_input_all_np=np.loadtxt(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\input_all_\"+str(i)+\".txt\",delimiter=\" \")\n",
    "large_output_all_np=np.load(base_path+\"output_y_all.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行分割\n",
    "input_1=[]  #268\n",
    "input_2=[]  #408\n",
    "input_3=[]  #278-398\n",
    "output_1=[] #268\n",
    "output_2=[] #408\n",
    "output_3=[] #278-398\n",
    "input_1_np=np.zeros((199*200*7,23))\n",
    "input_2_np=np.zeros((199*200*7,23))\n",
    "input_3_np=np.zeros((199*200*7*13,23))\n",
    "output_1_np=np.zeros((199*200*7,1))\n",
    "output_2_np=np.zeros((199*200*7,1))\n",
    "output_3_np=np.zeros((199*200*7*13,1))\n",
    "k=0\n",
    "j=0\n",
    "for i in range(0,15*7,15):\n",
    "    print(k)\n",
    "    #input_1.append(large_input_all_np[1*15*200*199:-1*15*200*199][i*199*200:(i+1)*199*200].flatten())\n",
    "    input_1_np[k*199*200:(k+1)*199*200]=large_input_all_np[1*15*200*199:-1*15*200*199][i*199*200:(i+1)*199*200]\n",
    "    #input_2.append(large_input_all_np[1*15*200*199:-1*15*200*199][(i+14)*199*200:(i+15)*199*200].flatten())\n",
    "    input_2_np[k*199*200:(k+1)*199*200]=large_input_all_np[1*15*200*199:-1*15*200*199][(i+14)*199*200:(i+15)*199*200]\n",
    "    #input_3.append(large_input_all_np[1*15*200*199:-1*15*200*199][(i+1)*199*200:(i+14)*199*200].flatten())\n",
    "    input_3_np[j*199*200:(j+13)*199*200]=large_input_all_np[1*15*200*199:-1*15*200*199][(i+1)*199*200:(i+14)*199*200]\n",
    "    #output_1.append(large_output_all_np[1*15*200*199:-1*15*200*199][i*199*200:(i+1)*199*200].flatten())\n",
    "    output_1_np[k*199*200:(k+1)*199*200]=large_output_all_np[1*15*200*199:-1*15*200*199][i*199*200:(i+1)*199*200]\n",
    "    #output_2.append(large_output_all_np[1*15*200*199:-1*15*200*199][(i+14)*199*200:(i+15)*199*200].flatten())\n",
    "    output_2_np[k*199*200:(k+1)*199*200]=large_output_all_np[1*15*200*199:-1*15*200*199][(i+14)*199*200:(i+15)*199*200]\n",
    "    #output_3.append(large_output_all_np[1*15*200*199:-1*15*200*199][(i+1)*199*200:(i+14)*199*200].flatten())\n",
    "    output_3_np[j*199*200:(j+13)*199*200]=large_output_all_np[1*15*200*199:-1*15*200*199][(i+1)*199*200:(i+14)*199*200]\n",
    "    k=k+1\n",
    "    j=j+13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练数据集的建立\n",
    "large_input_all_np=np.zeros((input_3_np.shape[0],8))\n",
    "large_input_all_np[:,0]=input_3_np[:,5]\n",
    "large_input_all_np[:,1]=input_3_np[:,11]/input_3_np[:,18]\n",
    "large_input_all_np[:,2]=input_3_np[:,17]\n",
    "large_input_all_np[:,3]=input_3_np[:,19]\n",
    "large_input_all_np[:,4]=input_3_np[:,20]\n",
    "large_input_all_np[:,5]=input_3_np[:,21]\n",
    "large_input_all_np[:,6]=input_3_np[:,22]\n",
    "large_input_all_np[:,7]=input_3_np[:,11]\n",
    "#训练数据集归一化的建立\n",
    "large_input_new=copy.deepcopy(large_input_all_np)\n",
    "large_output_new=copy.deepcopy(output_3_np)\n",
    "large_input_new_nor=np.zeros_like(large_input_new)\n",
    "large_output_new_nor=np.zeros_like(large_output_new)\n",
    "nor_num=[]\n",
    "#归一化参数的获取\n",
    "for i in range(8):\n",
    "    if i==0:\n",
    "        print(i,np.min(np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)))\n",
    "        nor_num.append(np.min(np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)))\n",
    "    elif i==1:\n",
    "        print(i,np.min(np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)))\n",
    "        nor_num.append(np.min(np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)))\n",
    "    elif i>=2: \n",
    "        print(i,np.mean(large_input_new[:,i]),np.std(large_input_new[:,i]))\n",
    "        nor_num.append([np.mean(large_input_new[:,i]),np.std(large_input_new[:,i])])\n",
    "#对数据进行归一化\n",
    "for i in range(8):\n",
    "    if i==0:\n",
    "        large_input_new_nor[:,i]=np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)/np.min(np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5))\n",
    "        #large_input_new_nor[:,i]=(large_input_new_nor[:,i]-np.mean(large_input_new_nor[:,i]))/np.std(large_input_new_nor[:,i])\n",
    "    elif i==1:\n",
    "        large_input_new_nor[:,i]=np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)/np.min(np.log10(large_input_new[:,i]+abs(np.min(large_input_new[:,i]))+1e-5))\n",
    "        #large_input_new_nor[:,i]=(large_input_new_nor[:,i]-np.mean(large_input_new_nor[:,i]))/np.std(large_input_new_nor[:,i])\n",
    "    elif i>=2:\n",
    "        large_input_new_nor[:,i]=(large_input_new[:,i]-np.mean(large_input_new[:,i]))/np.std(large_input_new[:,i])\n",
    "large_output_new_nor[:,0]=copy.deepcopy(large_output_new[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N12=199#N_EXTEND\n",
    "N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "total_num=range(int(len(large_input_new_nor)/N12))\n",
    "train_num=np.load(base_path+\"train_num.npy\").tolist()\n",
    "test_num=np.load(base_path+\"test_num.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立不同的神经网络\n",
    "hidden_size_4=[[8,8],[16,16],[24,24],[32,32]]\n",
    "hidden_size_5=[[8,8,8],[16,16,16],[24,24,24],[32,32,32]]\n",
    "hidden_size_6=[[8,8,8,8],[16,16,16,16],[24,24,24,24],[32,32,32,32]]\n",
    "hidden_size_7=[[8,8,8,8,8],[16,16,16,16,16],[24,24,24,24,24],[32,32,32,32,32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不同结构的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tanh\n",
    "for i in range(0,1):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            \"\"\"\n",
    "            我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            \"\"\"\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.Tanh(),#nn.Relu()#nn.Tanh()nn.Sigmoid()\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[1], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible\n",
    "            \"\"\"\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_4[0], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"4_layer_tanh\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_4_layer_tanh\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The tanh layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Relu\n",
    "for i in range(0,1):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            \"\"\"\n",
    "            我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            \"\"\"\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.ReLU(),#nn.Relu()#nn.Tanh()nn.Sigmoid()\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size[1], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible\n",
    "            \"\"\"\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_4[0], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"4_layer_relu\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_4_layer_relu\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The relu layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sigmoid\n",
    "for i in range(0,1):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            \"\"\"\n",
    "            我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            \"\"\"\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.Sigmoid(),#nn.Relu()#nn.Tanh()nn.Sigmoid()\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(hidden_size[1], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible\n",
    "            \"\"\"\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_4[0], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"4_layer_sigmoid\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_4_layer_sigmoid\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The sigmoid layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不同网络结构的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#四层网络\n",
    "for i in range(0,4):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            \"\"\"\n",
    "            我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            \"\"\"\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.Tanh(),#nn.Relu()#nn.Tanh()nn.Sigmoid()\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[1], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible\n",
    "            \"\"\"\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_4[i], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"4_layer_\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_4_layer\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The four layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##五层网络\n",
    "for i in range(4):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            #我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[2], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            #在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_5[i], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"5_layer_\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_5_layer\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The five layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##六层网络\n",
    "for i in range(3,4):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            #我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.Tanh(),#nn.Relu()#nn.Tanh()nn.Sigmoid()\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[2], hidden_size[3]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[3], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_6[i], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"6_layer_\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_6_layer\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The six layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##七层网络\n",
    "for i in range(3,4):\n",
    "    loss_append=[]\n",
    "    class TwoLayerNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            #我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.twolayernet = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size[0]),\n",
    "                nn.Tanh(),#nn.Relu()#nn.Tanh()nn.Sigmoid()\n",
    "                nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[2], hidden_size[3]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[3], hidden_size[4]),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size[4], output_size),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible\n",
    "            \"\"\"\n",
    "            y_pred = self.twolayernet(x)\n",
    "            return y_pred\n",
    "    epoch_0=32\n",
    "    input_size=8\n",
    "    output_size=1\n",
    "    model = TwoLayerNet(input_size, hidden_size_7[i], output_size)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    EPOCH =int(N1/epoch_0*6000)\n",
    "    begin_time = time()\n",
    "    N1=int(len(large_input_new_nor)/N12*0.8)\n",
    "    for t in range(EPOCH):      \n",
    "        # 向前传播\n",
    "        x1=random.sample(train_num,epoch_0)\n",
    "        z1=np.zeros((N12*epoch_0,input_size))\n",
    "        z2=np.zeros((N12*epoch_0,1))\n",
    "        k=0\n",
    "        for j in x1:\n",
    "            z1[k*N12:(k+1)*N12]=large_input_new_nor[j*N12:(j+1)*N12]\n",
    "            z2[k*N12:(k+1)*N12]=large_output_new_nor[j*N12:(j+1)*N12]\n",
    "            k=k+1\n",
    "        xx=Variable(torch.from_numpy(z1).to(torch.float32)) \n",
    "        yy=Variable(torch.from_numpy(z2).to(torch.float32))  \n",
    "        y_pred= model(xx)\n",
    "        # 计算损失\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        # 显示损失\n",
    "        loss_append.append(loss.item())\n",
    "        if (t+1) % int(N1/epoch_0*10)== 0:\n",
    "            print((t+1)/int(N1/epoch_0*10),loss.item())\n",
    "        # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "    end_time = time()\n",
    "    run_time = end_time-begin_time\n",
    "    print ('该循环程序运行时间:',run_time) #该循环程序运行时间： 1.4201874\n",
    "    PATH=\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\\"+\"7_layer_\"+str(i)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #loss_total.append(loss_append)\n",
    "    np.save(\"E:\\\\CPFEM\\\\CPFEM_ANN\\\\Article\\\\model\\\\loss_7_layer\"+str(i), loss_append)\n",
    "print(\"________________________________________________\")\n",
    "print(\"The seven layer has been successfully completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 1)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试数据集的误差分析\n",
    "error_sum=[]\n",
    "for test_n in train_num:#[9]\n",
    "    #test_n=N1+200\n",
    "    text_x=(Variable(torch.from_numpy(large_input_new_nor[test_n*N12:(test_n+1)*N12]).to(torch.float32)))\n",
    "    test_y=(large_output_new_nor[test_n*N12:(test_n+1)*N12])#*del_stress_std+del_stress_mean\n",
    "    test_error=np.hstack(((model(text_x).detach().numpy())-test_y))#*del_stress_std+del_stress_mean\n",
    "    error_sum.append(np.max(abs(test_error)))\n",
    "error_sum=[]\n",
    "for test_n in test_num:#[9]\n",
    "    #test_n=N1+200\n",
    "    text_x=(Variable(torch.from_numpy(large_input_new_nor[test_n*N12:(test_n+1)*N12]).to(torch.float32)))\n",
    "    test_y=(large_output_new_nor[test_n*N12:(test_n+1)*N12])#*del_stress_std+del_stress_mean\n",
    "    test_error=np.hstack(((model(text_x).detach().numpy())-test_y))#*del_stress_std+del_stress_mean\n",
    "    error_sum.append(np.max(abs(test_error)))\n",
    "plt.plot(error_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试数据集\n",
    "input_1_S_np=copy.deepcopy(large_input_all_np[0:1*15*200*199])  #1E-2\n",
    "output_1_S_np=copy.deepcopy(large_output_all_np[0:1*15*200*199])\n",
    "#测试数据集的重新分布\n",
    "large_input_all_np_1_S=np.zeros((input_1_S_np.shape[0],8))\n",
    "large_input_all_np_1_S[:,0]=input_1_S_np[:,5]\n",
    "large_input_all_np_1_S[:,1]=input_1_S_np[:,11]/input_1_S_np[:,18]\n",
    "large_input_all_np_1_S[:,2]=input_1_S_np[:,17]\n",
    "large_input_all_np_1_S[:,3]=input_1_S_np[:,19]\n",
    "large_input_all_np_1_S[:,4]=input_1_S_np[:,20]\n",
    "large_input_all_np_1_S[:,5]=input_1_S_np[:,21]\n",
    "large_input_all_np_1_S[:,6]=input_1_S_np[:,22]\n",
    "large_input_all_np_1_S[:,7]=input_1_S_np[:,11]\n",
    "#数据归一化\n",
    "large_input_new_1_S=copy.deepcopy(large_input_all_np_1_S)\n",
    "large_output_new_1_S=copy.deepcopy(output_1_S_np)\n",
    "large_input_new_nor_1_S=np.zeros_like(large_input_new_1_S)\n",
    "large_output_new_nor_1_S=np.zeros_like(large_output_new_1_S)\n",
    "large_output_new_nor_1_S[:,0]=copy.deepcopy(large_output_new_1_S[:,0])\n",
    "for i in range(8):\n",
    "    if i==0:\n",
    "        large_input_new_nor_1_S[:,i]=np.log10(large_input_new_1_S[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)/nor_num[0]\n",
    "        #large_input_new_nor[:,i]=(large_input_new_nor[:,i]-np.mean(large_input_new_nor[:,i]))/np.std(large_input_new_nor[:,i])\n",
    "    elif i==1:\n",
    "        large_input_new_nor_1_S[:,i]=np.log10(large_input_new_1_S[:,i]+abs(np.min(large_input_new[:,i]))+1e-5)/nor_num[1]\n",
    "        #large_input_new_nor[:,i]=(large_input_new_nor[:,i]-np.mean(large_input_new_nor[:,i]))/np.std(large_input_new_nor[:,i])\n",
    "    elif i>=2:\n",
    "        large_input_new_nor_1_S[:,i]=(large_input_new_1_S[:,i]-nor_num[i][0])/nor_num[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单晶应力的计算\n",
    "def cal_stress(delt_sigma):\n",
    "    large_output_new_cal=np.zeros(200)\n",
    "    for i in range(1,200):\n",
    "        large_output_new_cal[i]=np.sum(delt_sigma[0:i])\n",
    "    return large_output_new_cal\n",
    "#多晶应力的计算\n",
    "def poly_stress(delt_sigma):\n",
    "    poly_sum=np.zeros(200)\n",
    "    for kk in range(200):\n",
    "        poly_sum=poly_sum+cal_stress(delt_sigma[kk*199:(kk+1)*199])\n",
    "    return poly_sum/200\n",
    "#多晶应力的计算\n",
    "def poly_strain(delt_sigma):\n",
    "    poly_sum=np.zeros(199)\n",
    "    for kk in range(200):\n",
    "        poly_sum=poly_sum+delt_sigma[kk*199:(kk+1)*199]\n",
    "    return poly_sum/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp=\"rbgymc\"\n",
    "font1 = {'family': 'Times New Roman',\n",
    "         'weight': 'normal',\n",
    "         'size': 15,\n",
    "         }\n",
    "font2 = {'family': 'Times New Roman',\n",
    "         'weight': 'normal',\n",
    "         'size': 12,\n",
    "         }\n",
    "matplotlib.rcParams['xtick.direction'] = \"in\"\n",
    "matplotlib.rcParams['ytick.direction'] = \"in\"\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "figsize=(6,4)\n",
    "figure, ax = plt.subplots(figsize=figsize)\n",
    "plt.xlabel('Strain',font1)\n",
    "plt.ylabel('Stress',font1)\n",
    "labels = ax.get_xticklabels() + ax.get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]\n",
    "#plt.xticks(np.linspace(-5, -3, 5))\n",
    "#plt.yticks(np.linspace(0, 100, 11))\n",
    "#plt.axis((-0, 0.0003, 0, 300000))\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.axis((-0.001, 0.03, 0, 180))\n",
    "kk=0\n",
    "for t in [0,4,8,13]:\n",
    "    text_x=(Variable(torch.from_numpy(large_input_new_nor_1_S[t*199*200:(t+1)*199*200]).to(torch.float32)))\n",
    "    plt.plot(poly_strain(large_input_all_np_1_S[:,0][t*199*200:(t+1)*199*200])[0:199:4],\n",
    "         poly_stress(large_output_new_1_S[t*199*200:(t+1)*199*200])[0:-1][0:199:4],\"cD-\",\n",
    "             linewidth=0.2,c=cmp[kk])\n",
    "    plt.plot(poly_strain(large_input_all_np_1_S[:,0][t*199*200:(t+1)*199*200]),\n",
    "         poly_stress((model(text_x).detach().numpy())[:,0])[0:-1],\n",
    "             label=str(temperature_txt[t])+\"℃ \"+strain_rate_txt[0]+\"/s\",c=cmp[kk])\n",
    "    kk=kk+1\n",
    "plt.legend(loc='center', bbox_to_anchor=(0.82,0.82),prop=font2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:orange3]",
   "language": "python",
   "name": "conda-env-orange3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
